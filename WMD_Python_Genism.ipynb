{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WMD Python Genism.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaunakSen/Contextual-hyperlinks/blob/master/WMD_Python_Genism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1FQgEEn-KKS",
        "colab_type": "text"
      },
      "source": [
        "## Finding similar documents with Word2Vec and WMD\n",
        "\n",
        "[link](https://markroxor.github.io/gensim/static/notebooks/WMD_tutorial.html)\n",
        "\n",
        "Word Mover's Distance is a promising new tool in machine learning that allows us to submit a query and return the most relevant documents. For example, in a blog post OpenTable use WMD on restaurant reviews. Using this approach, they are able to mine different aspects of the reviews. In part 2 of this tutorial, we show how you can use Gensim's WmdSimilarity to do something similar to what OpenTable did. In part 1 shows how you can compute the WMD distance between two documents using wmdistance. Part 1 is optional if you want use WmdSimilarity, but is also useful in it's own merit.\n",
        "\n",
        "### Word Mover's Distance basics\n",
        "\n",
        "WMD is a method that allows us to assess the \"distance\" between two documents in a meaningful way, even when they have no words in common. It uses word2vec [4] vector embeddings of words. It been shown to outperform many of the state-of-the-art methods in k-nearest neighbors classification [3].\n",
        "\n",
        "WMD is illustrated below for two very similar sentences (illustration taken from Vlad Niculae's blog). The sentences have no words in common, but by matching the relevant words, WMD is able to accurately measure the (dis)similarity between the two sentences. The method also uses the bag-of-words representation of the documents (simply put, the word's frequencies in the documents), noted as  d  in the figure below. The intution behind the method is that we find the minimum \"traveling distance\" between documents, in other words the most efficient way to \"move\" the distribution of document 1 to the distribution of document 2.\n",
        "\n",
        "![](https://vene.ro/images/wmd-obama.png)\n",
        "\n",
        "This method was introduced in the article \"From Word Embeddings To Document Distances\" by Matt Kusner et al. (link to PDF). It is inspired by the \"Earth Mover's Distance\", and employs a solver of the \"transportation problem\".\n",
        "\n",
        "In this tutorial, we will learn how to use Gensim's WMD functionality, which consists of the wmdistance method for distance computation, and the WmdSimilarity class for corpus based similarity queries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0diEFbp_MsS",
        "colab_type": "text"
      },
      "source": [
        "### Part 1: Computing the Word Mover's Distance\n",
        "\n",
        "To use WMD, we need some word embeddings first of all. You could train a word2vec (see tutorial here) model on some corpus, but we will start by downloading some pre-trained word2vec embeddings. Download the GoogleNews-vectors-negative300.bin.gz embeddings here (warning: 1.5 GB, file is not needed for part 2). Training your own embeddings can be beneficial, but to simplify this tutorial, we will be using pre-trained embeddings at first.\n",
        "\n",
        "Let's take some sentences to compute the distance between.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L2TSOvn9IfE",
        "colab_type": "code",
        "outputId": "deea8f79-70b4-4b38-e36b-d7da505af0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-09-04 21:28:43--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.36.110\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.36.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  34.0MB/s    in 43s     \n",
            "\n",
            "2019-09-04 21:29:27 (36.1 MB/s) - ‘GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTFFwqxt9NCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "start_nb = time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kLbqN0d_whI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize logging.\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s')\n",
        "\n",
        "sentence_obama = 'Obama speaks to the media in Illinois'\n",
        "sentence_president = 'The president greets the press in Chicago'\n",
        "sentence_obama = sentence_obama.lower().split()\n",
        "sentence_president = sentence_president.lower().split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzF2mDKj_0LJ",
        "colab_type": "text"
      },
      "source": [
        "These sentences have very similar content, and as such the WMD should be low. Before we compute the WMD, we want to remove stopwords (\"the\", \"to\", etc.), as these do not contribute a lot to the information in the sentences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTHt9OCTuJgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSMR6r98uJZt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "571f80a1-46e6-4b6f-a72b-9c818eb6ceaa"
      },
      "source": [
        "!pip install nltk==3.4.4"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.4.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.4.4) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.4.4-cp36-none-any.whl size=1450224 sha256=c6dd7f2acf7927706f22248415bc625874efd11f070f9c1b124b3576fe0fe811\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/c8/31/48ace4468e236e0e8435f30d33e43df48594e4d53e367cf061\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.4.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbdFp449_x28",
        "colab_type": "code",
        "outputId": "2f283a43-c9e7-44f3-efc2-48adb7f36967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        }
      },
      "source": [
        "# Import and download stopwords from NLTK.\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "\n",
        "download('stopwords') # Download stopwords list."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njMeMFhTAEM0",
        "colab_type": "code",
        "outputId": "4f049169-c9c2-48bb-d912-c68a52ad3e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print (stopwords.words('english')[:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM2kwfp7AI1H",
        "colab_type": "code",
        "outputId": "a5916f23-f4eb-497c-db38-3bd4081671c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Remove stopwords.\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "sentence_obama = [word for word in sentence_obama if word not in stopwords]\n",
        "sentence_president = [word for word in sentence_president if word not in stopwords]\n",
        "\n",
        "print (sentence_obama)\n",
        "print (sentence_president)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['obama', 'speaks', 'media', 'illinois']\n",
            "['president', 'greets', 'press', 'chicago']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pluaM7j7Dsin",
        "colab_type": "text"
      },
      "source": [
        "Now, as mentioned earlier, we will be using some downloaded pre-trained embeddings. We load these into a Gensim Word2Vec model class. Note that the embeddings we have chosen here require a lot of memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ue2oOJbDyCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkT2pYZ4A7Lw",
        "colab_type": "code",
        "outputId": "4dbb64d6-1874-4753-efbf-91fc57a2744c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "start = time()\n",
        "\n",
        "if not os.path.exists(path=\"./GoogleNews-vectors-negative300.bin.gz\"):\n",
        "  raise ValueError(\"SKIP: You need to download the google news model\")\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(fname='./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
        "\n",
        "print('Cell took %.2f seconds to run.' % (time() - start))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cell took 117.37 seconds to run.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw_uleUSFkgN",
        "colab_type": "text"
      },
      "source": [
        "So let's compute WMD using the wmdistance method.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZaIa8Z0E4za",
        "colab_type": "code",
        "outputId": "9a1c1754-cc93-44f6-8b21-fa2ec1b78042",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "distance = model.wmdistance(sentence_obama, sentence_president)\n",
        "print (distance)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.3741233214730024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqdWEO_2Fqmh",
        "colab_type": "code",
        "outputId": "b38b7ebd-61ac-475c-c533-8b54af7b312a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Let's try the same thing with two completely unrelated sentences. Notice that the distance is larger.\n",
        "\n",
        "sentence_orange = 'Oranges are my favorite fruit'\n",
        "sentence_orange = sentence_orange.lower().split()\n",
        "sentence_orange = [word for word in sentence_orange if word not in stopwords]\n",
        "\n",
        "print (model.wmdistance(document1=sentence_obama, document2=sentence_orange))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.380239402988511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eKv-hyriz8n",
        "colab_type": "text"
      },
      "source": [
        "#### Normalizing word2vec vectors\n",
        "\n",
        "When using the wmdistance method, it is beneficial to normalize the word2vec vectors first, so they all have equal length. To do this, simply call model.init_sims(replace=True) and Gensim will take care of that for you.\n",
        "\n",
        "Usually, one measures the distance between two word2vec vectors using the cosine distance (see cosine similarity), which measures the angle between vectors. WMD, on the other hand, uses the Euclidean distance. The Euclidean distance between two vectors might be large because their lengths differ, but the cosine distance is small because the angle between them is small; we can mitigate some of this by normalizing the vectors.\n",
        "\n",
        "Note that normalizing the vectors can take some time, especially if you have a large vocabulary and/or large vectors.\n",
        "\n",
        "Usage is illustrated in the example below. It just so happens that the vectors we have downloaded are already normalized, so it won't do any difference in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XgjjzfiG7G1",
        "colab_type": "code",
        "outputId": "dc8f7fe0-653e-44e0-e9c7-d13bf2b84f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "# Normalizing word2vec vectors.\n",
        "start = time()\n",
        "\n",
        "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
        "print ('Cell took %.2f seconds to run.' %(time() - start))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cell took 27.60 seconds to run.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TqRnYIHjMQS",
        "colab_type": "code",
        "outputId": "c8c06fa9-64b2-4d89-c2fb-809b7cec49ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(model.wmdistance(sentence_obama, sentence_president))  # Compute WMD as normal.\n",
        "\n",
        "print (model.wmdistance(sentence_orange, sentence_obama))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0174646259300113\n",
            "1.3663488311444436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqVJ8iOJjXoz",
        "colab_type": "code",
        "outputId": "1feef546-a146-48cc-c0c4-5e111e475871",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "real_captions = ['two dogs playing in the field', 'man in red shirt riding bike', 'little bird sitting on a branch', 'man in blue is in the water']\n",
        "\n",
        "generated_captions = ['puppies running in the ground', 'man riding bicycle in maroon', 'bird sits in leafless tree', 'child in black wetsuit is in the waves on surfboard']\n",
        "\n",
        "real_captions = [word for word in real_captions if word not in stopwords]\n",
        "generated_captions = [word for word in generated_captions if word not in stopwords]\n",
        "\n",
        "wrong_captions = ['a computer on the floor'] * len(real_captions)\n",
        "\n",
        "wrong_captions = [word for word in wrong_captions if word not in stopwords]\n",
        "\n",
        "\n",
        "for x in range(len(real_captions)):\n",
        "  print (model.wmdistance(real_captions[x], generated_captions[x]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9862614148723878\n",
            "1.0401546156734276\n",
            "1.0679314297375004\n",
            "0.8883583908601389\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E05di8TcUAKJ",
        "colab_type": "code",
        "outputId": "f54fb89c-0570-4669-c0c5-ff5d6bcdf257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "for x in range(len(real_captions)):\n",
        "  print (model.wmdistance(real_captions[x], wrong_captions[x]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.28264269547198\n",
            "1.5681078434373743\n",
            "1.3253273516472157\n",
            "1.1607600246182723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1tAwlp-U7iA",
        "colab_type": "text"
      },
      "source": [
        "Better idea to remove the stopwords and then try"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXTcKZKXsUHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "53012222-871d-49f0-ba83-ff020257fa83"
      },
      "source": [
        "idx = 3\n",
        "reference = [[word for word in real_captions[idx].split()]]\n",
        "candidate = [word for word in generated_captions[idx].split()]\n",
        "print (reference, candidate)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['child', 'in', 'black', 'wetsuit', 'is', 'in', 'the', 'waves', 'on', 'surfboard']] ['man', 'in', 'blue', 'is', 'in', 'the', 'water']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmRL4h2TrVjA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "e2580bd0-11e5-4ffa-b93e-8c7c8df29140"
      },
      "source": [
        "# cumulative BLEU scores\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=None))\n",
        "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0), smoothing_function=None))\n",
        "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0), smoothing_function=None))\n",
        "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0,0,0,1)))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulative 1-gram: 0.372251\n",
            "Cumulative 2-gram: 0.217146\n",
            "Cumulative 3-gram: 0.130288\n",
            "Cumulative 4-gram: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}